#!/usr/bin/env python3
"""To use this module with your actual results:

1. Load your results from JSON files:
   ```python
   from enhanced_layer_visualization import load_results_from_files, create_enhanced_layer_importance_plot

   result_files = ["model1_results.json", "model2_results.json"]
   results = load_results_from_files(result_files)
   ```

2. Create enhanced visualizations:
   ```python
   fig = create_enhanced_layer_importance_plot(
       results_list=results,
       metrics=['AD', 'FOC'],
       save_path="my_analysis.png"
   )
   ```

3. Generate statistical summaries:
   ```python
   from enhanced_layer_visualization import generate_statistical_summary

   summary_df = generate_statistical_summary(results)
   print(summary_df)
   ```

The module expects results in the format generated by the main layer importance analysis code.
All visualizations are publication-ready with professional styling and confidence intervals.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import json
import math
from typing import List, Dict, Any, Optional, Tuple, Union
from scipy import stats
from scipy.stats import pearsonr
import warnings
warnings.filterwarnings("ignore")


plt.style.use('default')
sns.set_style("whitegrid")
plt.rcParams.update({
    'font.size': 12,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 11,
    'figure.titlesize': 16,
    'font.family': 'serif',
    'font.serif': ['Times New Roman'],
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.grid': True,
    'grid.alpha': 0.3,
})


MODEL_COLORS = {
    'math-instruct': '#2E86AB',  # Blue - Instruction-tuned models
    'math-rl': '#A23B72',       # Purple - RL-trained models
    'r1-distill': '#F18F01',    # Orange - Distilled models
    'default': '#43AA8B',       # Teal - Default fallback
}

# Additional colors for multiple models
EXTENDED_COLORS = ['#2E86AB', '#A23B72', '#F18F01', '#43AA8B', '#C73E1D', '#7209B7']


def extract_model_type(model_name: str) -> str:
    """
    Extract model type from full model name for consistent styling.

    Args:
        model_name: Full HuggingFace model identifier

    Returns:
        Simplified model type for color mapping
    """
    name_lower = model_name.lower()

    if 'math' in name_lower and 'instruct' in name_lower:
        return 'math-instruct'
    elif 'math' in name_lower and 'rl' in name_lower:
        return 'math-rl'
    elif 'r1' in name_lower and 'distill' in name_lower:
        return 'r1-distill'
    else:
        return 'default'


def clean_model_name(model_name: str) -> str:
    """
    Clean model name for display in legends and titles.

    Args:
        model_name: Full model name or path

    Returns:
        Cleaned, display-friendly model name
    """
    # Extract just the model name from the path
    clean_name = model_name.split("/")[-1] if "/" in model_name else model_name

    # Remove common prefixes and clean up
    clean_name = clean_name.replace("deepseek-", "").replace("DeepSeek-", "")
    clean_name = clean_name.replace("-7b", "").replace("-7B", "")
    clean_name = clean_name.replace("_", "-")

    return clean_name


def compute_regression_stats(x_data: np.ndarray, y_data: np.ndarray,
                           confidence_level: float = 0.95) -> Dict[str, Any]:
    """
    Compute comprehensive regression statistics including confidence intervals.

    This function performs linear regression analysis and computes confidence
    intervals for both the regression line and prediction intervals.

    Args:
        x_data: Independent variable (layer indices)
        y_data: Dependent variable (AD or FOC values)
        confidence_level: Confidence level for intervals (default 0.95)

    Returns:
        Dictionary containing regression statistics and confidence intervals
    """
    # Remove any NaN values
    valid_mask = ~(np.isnan(x_data) | np.isnan(y_data))
    x_clean = x_data[valid_mask]
    y_clean = y_data[valid_mask]

    if len(x_clean) < 3:
        # Not enough data points for reliable regression
        return {
            'slope': np.nan, 'intercept': np.nan, 'r_value': np.nan,
            'p_value': np.nan, 'std_err': np.nan, 'n_points': len(x_clean),
            'x_pred': np.array([]), 'y_pred': np.array([]),
            'ci_lower': np.array([]), 'ci_upper': np.array([]),
            'valid': False
        }

    # Perform linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean)

    # Generate prediction points for smooth regression line
    x_min, x_max = x_clean.min(), x_clean.max()
    x_pred = np.linspace(x_min, x_max, 100)
    y_pred = slope * x_pred + intercept

    # Compute confidence intervals
    # Standard error of prediction
    n = len(x_clean)
    x_mean = np.mean(x_clean)

    # Sum of squares
    sxx = np.sum((x_clean - x_mean) ** 2)
    sxy = np.sum((x_clean - x_mean) * (y_clean - np.mean(y_clean)))
    syy = np.sum((y_clean - np.mean(y_clean)) ** 2)

    # Residual sum of squares
    y_pred_data = slope * x_clean + intercept
    residuals = y_clean - y_pred_data
    rss = np.sum(residuals ** 2)
    mse = rss / (n - 2) if n > 2 else 0

    # t-statistic for confidence level
    alpha = 1 - confidence_level
    t_val = stats.t.ppf(1 - alpha/2, n - 2) if n > 2 else 2.0

    # Standard error for each prediction point
    se_pred = np.sqrt(mse * (1/n + (x_pred - x_mean)**2 / sxx))

    # Confidence intervals
    ci_lower = y_pred - t_val * se_pred
    ci_upper = y_pred + t_val * se_pred

    return {
        'slope': slope,
        'intercept': intercept,
        'r_value': r_value,
        'p_value': p_value,
        'std_err': std_err,
        'n_points': n,
        'x_pred': x_pred,
        'y_pred': y_pred,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'valid': True,
        'mse': mse
    }


def create_enhanced_layer_importance_plot(results_list: List[Dict[str, Any]],
                                        metrics: List[str] = ['AD', 'FOC'],
                                        figsize: Tuple[int, int] = (16, 8),
                                        confidence_level: float = 0.95,
                                        show_stats: bool = True,
                                        save_path: Optional[str] = None,
                                        dpi: int = 300) -> plt.Figure:
    """
    Create enhanced layer importance visualization with regression lines and confidence intervals.

    This function generates publication-quality plots showing layer importance metrics
    with best-fit regression lines and confidence bands, reproducing the style of
    Figure 2 from the paper.

    Args:
        results_list: List of evaluation results from layer importance analysis
        metrics: List of metrics to plot ('AD' and/or 'FOC')
        figsize: Figure size as (width, height) tuple
        confidence_level: Confidence level for regression intervals
        show_stats: Whether to display statistical annotations
        save_path: Optional path to save the figure
        dpi: Resolution for saved figure

    Returns:
        matplotlib Figure object
    """
    # Filter successful results
    successful_results = [r for r in results_list if r.get("success", False)]

    if not successful_results:
        print("No successful results to plot!")
        return None

    # Create subplot structure
    n_metrics = len(metrics)
    fig, axes = plt.subplots(1, n_metrics, figsize=figsize)
    if n_metrics == 1:
        axes = [axes]

    # Process each metric
    for metric_idx, metric in enumerate(metrics):
        ax = axes[metric_idx]

        # Track statistics for annotation
        model_stats = []

        # Plot each model
        for model_idx, results in enumerate(successful_results):
            model_name = results["model_name"]
            clean_name = clean_model_name(model_name)
            model_type = extract_model_type(model_name)
            color = MODEL_COLORS.get(model_type, EXTENDED_COLORS[model_idx % len(EXTENDED_COLORS)])

            # Extract layer metrics
            layer_metrics = results.get("layer_metrics", [])
            if not layer_metrics:
                continue

            # Extract metric-specific data
            layers = []
            values = []
            ci_lower = []
            ci_upper = []

            for m in layer_metrics:
                layer = m.get("layer")
                value = m.get(metric, np.nan)

                if layer is not None and not math.isnan(value):
                    layers.append(layer)
                    values.append(value)

                    # Extract confidence intervals if available
                    ci_low = m.get(f"{metric}_low", np.nan)
                    ci_high = m.get(f"{metric}_high", np.nan)
                    ci_lower.append(ci_low if not math.isnan(ci_low) else value)
                    ci_upper.append(ci_high if not math.isnan(ci_high) else value)

            if len(layers) < 2:
                continue

            # Convert to numpy arrays
            x_data = np.array(layers)
            y_data = np.array(values)
            ci_low_data = np.array(ci_lower)
            ci_high_data = np.array(ci_upper)

            # Plot data points with error bars (if available)
            has_error_bars = not np.allclose(ci_low_data, y_data) or not np.allclose(ci_high_data, y_data)

            if has_error_bars and metric == 'FOC':
                ax.errorbar(x_data, y_data,
                          yerr=[y_data - ci_low_data, ci_high_data - y_data],
                          fmt='o', color=color, alpha=0.7, capsize=3, capthick=1.5,
                          markersize=6, label=f"{clean_name} (data)")
            else:
                # Simple scatter plot for AD or when no error bars
                ax.scatter(x_data, y_data, color=color, alpha=0.7, s=50,
                          label=f"{clean_name} (data)", zorder=3)

            # Compute regression statistics
            reg_stats = compute_regression_stats(x_data, y_data, confidence_level)

            if reg_stats['valid'] and len(x_data) >= 3:
                # Plot regression line
                ax.plot(reg_stats['x_pred'], reg_stats['y_pred'],
                       color=color, linewidth=2.5, alpha=0.9,
                       label=f"{clean_name} (r={reg_stats['r_value']:.3f})")

                # Plot confidence band
                ax.fill_between(reg_stats['x_pred'],
                              reg_stats['ci_lower'], reg_stats['ci_upper'],
                              color=color, alpha=0.2,
                              label=f"{clean_name} ({confidence_level:.0%} CI)")

                # Store statistics for annotation
                model_stats.append({
                    'name': clean_name,
                    'slope': reg_stats['slope'],
                    'r_value': reg_stats['r_value'],
                    'p_value': reg_stats['p_value'],
                    'n_points': reg_stats['n_points']
                })

        # Customize axis
        ax.set_xlabel("Layer Index", fontweight='bold')

        if metric == 'AD':
            ax.set_ylabel("Accuracy Drop (AD)", fontweight='bold')
            ax.set_title("Layer Importance: Accuracy Drop", fontweight='bold', pad=15)
            # AD can be negative, so allow auto-scaling

        elif metric == 'FOC':
            ax.set_ylabel("Flip-Out-of-Correct (FOC)", fontweight='bold')
            ax.set_title("Layer Importance: Flip-Out-of-Correct", fontweight='bold', pad=15)
            ax.set_ylim(-0.05, 1.05)  # FOC is bounded between 0 and 1

        # Add grid and styling
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)

        # Add legend
        if len(successful_results) <= 3:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left',
                     frameon=True, fancybox=True, shadow=True)
        else:
            # Compact legend for many models
            ax.legend(loc='best', frameon=True, fancybox=True, shadow=True, ncol=2)

        # Add statistical annotations if requested
        if show_stats and model_stats:
            stats_text = []
            for i, stats in enumerate(model_stats[:3]):
                slope_interpretation = "increasing" if stats['slope'] > 0 else "decreasing"
                significance = "***" if stats['p_value'] < 0.001 else "**" if stats['p_value'] < 0.01 else "*" if stats['p_value'] < 0.05 else ""

                stats_text.append(f"{stats['name']}: slope={stats['slope']:.4f}{significance}")

            if stats_text:
                ax.text(0.02, 0.98, '\n'.join(stats_text),
                       transform=ax.transAxes, fontsize=9,
                       verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3",
                       facecolor="white", alpha=0.8))


    plt.tight_layout()


    if n_metrics > 1:
        fig.suptitle("Layer Criticality Analysis with Regression Trends",
                    fontsize=16, fontweight='bold', y=0.98)
        plt.subplots_adjust(top=0.90)


    if save_path:
        try:
            plt.savefig(save_path, dpi=dpi, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            print(f"Enhanced figure saved to {save_path}")
        except Exception as e:
            print(f"Could not save figure: {e}")

    return fig


def create_correlation_heatmap(results_list: List[Dict[str, Any]],
                             save_path: Optional[str] = None) -> plt.Figure:
    """
    Create correlation heatmap showing layer-depth vs. metric correlations.

    This visualization helps identify patterns in how layer depth correlates
    with importance metrics across different models.

    Args:
        results_list: List of evaluation results
        save_path: Optional path to save the figure

    Returns:
        matplotlib Figure object
    """
    # Prepare correlation data
    correlation_data = []

    successful_results = [r for r in results_list if r.get("success", False)]

    for results in successful_results:
        model_name = clean_model_name(results["model_name"])
        layer_metrics = results.get("layer_metrics", [])

        if not layer_metrics:
            continue

        # Extract data for correlation analysis
        layers = []
        ad_values = []
        foc_values = []

        for m in layer_metrics:
            layer = m.get("layer")
            ad = m.get("AD")
            foc = m.get("FOC")

            if layer is not None and not math.isnan(ad) and not math.isnan(foc):
                layers.append(layer)
                ad_values.append(ad)
                foc_values.append(foc)

        if len(layers) >= 3:
            # Compute correlations
            ad_corr, ad_p = pearsonr(layers, ad_values)
            foc_corr, foc_p = pearsonr(layers, foc_values)

            correlation_data.append({
                'Model': model_name,
                'AD Correlation': ad_corr,
                'AD P-value': ad_p,
                'FOC Correlation': foc_corr,
                'FOC P-value': foc_p,
            })

    if not correlation_data:
        print("No valid correlation data to plot!")
        return None

    # Create DataFrame and heatmap
    df = pd.DataFrame(correlation_data)

    # Prepare data for heatmap
    heatmap_data = df[['AD Correlation', 'FOC Correlation']].T
    heatmap_data.columns = df['Model']

    # Create figure
    fig, ax = plt.subplots(figsize=(10, 6))

    # Create heatmap
    sns.heatmap(heatmap_data, annot=True, cmap='RdBu_r', center=0,
                square=True, cbar_kws={'label': 'Correlation Coefficient'},
                fmt='.3f', ax=ax)

    ax.set_title('Layer Depth vs. Metric Correlations', fontweight='bold', pad=15)
    ax.set_xlabel('Models', fontweight='bold')
    ax.set_ylabel('Metrics', fontweight='bold')

    plt.tight_layout()

    # Save if requested
    if save_path:
        try:
            plt.savefig(save_path, dpi=300, bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            print(f"Correlation heatmap saved to {save_path}")
        except Exception as e:
            print(f"Could not save heatmap: {e}")

    return fig


def generate_statistical_summary(results_list: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Generate comprehensive statistical summary of layer importance analysis.

    Args:
        results_list: List of evaluation results

    Returns:
        DataFrame with detailed statistical summary
    """
    summary_data = []

    successful_results = [r for r in results_list if r.get("success", False)]

    for results in successful_results:
        model_name = clean_model_name(results["model_name"])
        layer_metrics = results.get("layer_metrics", [])
        baseline_acc = results.get("metadata", {}).get("baseline_accuracy", np.nan)

        if not layer_metrics:
            continue

        # Extract layer data
        layers = []
        ad_values = []
        foc_values = []

        for m in layer_metrics:
            layer = m.get("layer")
            ad = m.get("AD")
            foc = m.get("FOC")

            if layer is not None:
                layers.append(layer)
                ad_values.append(ad if not math.isnan(ad) else np.nan)
                foc_values.append(foc if not math.isnan(foc) else np.nan)

        if not layers:
            continue

        # Compute statistics
        layers_arr = np.array(layers)
        ad_arr = np.array(ad_values)
        foc_arr = np.array(foc_values)

        # Filter valid values
        valid_ad = ad_arr[~np.isnan(ad_arr)]
        valid_foc = foc_arr[~np.isnan(foc_arr)]
        valid_layers_ad = layers_arr[~np.isnan(ad_arr)]
        valid_layers_foc = layers_arr[~np.isnan(foc_arr)]

        # Correlation analysis
        ad_corr = pearsonr(valid_layers_ad, valid_ad)[0] if len(valid_ad) >= 3 else np.nan
        foc_corr = pearsonr(valid_layers_foc, valid_foc)[0] if len(valid_foc) >= 3 else np.nan

        # Peak analysis
        max_ad_idx = np.argmax(valid_ad) if len(valid_ad) > 0 else np.nan
        max_foc_idx = np.argmax(valid_foc) if len(valid_foc) > 0 else np.nan

        max_ad_layer = valid_layers_ad[max_ad_idx] if not math.isnan(max_ad_idx) else np.nan
        max_foc_layer = valid_layers_foc[max_foc_idx] if not math.isnan(max_foc_idx) else np.nan

        summary_data.append({
            'Model': model_name,
            'Baseline_Accuracy': f"{baseline_acc:.3f}" if not math.isnan(baseline_acc) else "N/A",
            'Num_Layers': len(layers),
            'AD_Mean': f"{np.nanmean(ad_arr):.4f}" if len(valid_ad) > 0 else "N/A",
            'AD_Std': f"{np.nanstd(ad_arr):.4f}" if len(valid_ad) > 0 else "N/A",
            'AD_Max': f"{np.nanmax(ad_arr):.4f}" if len(valid_ad) > 0 else "N/A",
            'AD_Peak_Layer': f"L{int(max_ad_layer)}" if not math.isnan(max_ad_layer) else "N/A",
            'AD_Correlation': f"{ad_corr:.3f}" if not math.isnan(ad_corr) else "N/A",
            'FOC_Mean': f"{np.nanmean(foc_arr):.4f}" if len(valid_foc) > 0 else "N/A",
            'FOC_Std': f"{np.nanstd(foc_arr):.4f}" if len(valid_foc) > 0 else "N/A",
            'FOC_Max': f"{np.nanmax(foc_arr):.4f}" if len(valid_foc) > 0 else "N/A",
            'FOC_Peak_Layer': f"L{int(max_foc_layer)}" if not math.isnan(max_foc_layer) else "N/A",
            'FOC_Correlation': f"{foc_corr:.3f}" if not math.isnan(foc_corr) else "N/A",
        })

    return pd.DataFrame(summary_data)


def load_results_from_files(file_paths: List[str]) -> List[Dict[str, Any]]:
    """
    Load evaluation results from JSON files.

    Args:
        file_paths: List of paths to JSON result files

    Returns:
        List of loaded results dictionaries
    """
    results = []

    for file_path in file_paths:
        try:
            with open(file_path, 'r') as f:
                result = json.load(f)
                results.append(result)
            print(f"Loaded results from {file_path}")
        except Exception as e:
            print(f"Could not load {file_path}: {e}")

    return results


def main_visualization_demo():
    """
    Demonstration function showing how to use the enhanced visualization tools.

    This function provides example usage of all visualization functions with
    sample data that matches the structure expected from the layer importance analysis.
    """
    print("ðŸŽ¨ Enhanced Layer Importance Visualization Demo")
    print("=" * 50)

    # Example: Load results from files (replace with actual file paths)
    # result_files = [
    #     "gsm8k_ablation_deepseek_math_7b_instruct_20samples.json",
    #     "gsm8k_ablation_deepseek_math_7b_rl_20samples.json"
    # ]
    #
    # results_list = load_results_from_files(result_files)

    # For demo purposes, create sample data structure
    print("ðŸ“Š Creating sample data structure...")

    # Sample data matching the actual output format
    sample_results = [
        {
            "model_name": "deepseek-ai/deepseek-math-7b-instruct",
            "success": True,
            "metadata": {
                "baseline_accuracy": 0.65,
                "num_layers": 32,
                "n_samples": 20
            },
            "layer_metrics": [
                {"layer": i, "AD": np.random.normal(0.02, 0.1), "FOC": np.random.beta(2, 5),
                 "FOC_low": max(0, np.random.beta(2, 5) - 0.1), "FOC_high": min(1, np.random.beta(2, 5) + 0.1)}
                for i in range(32)
            ]
        },
        {
            "model_name": "deepseek-ai/deepseek-math-7b-rl",
            "success": True,
            "metadata": {
                "baseline_accuracy": 0.72,
                "num_layers": 32,
                "n_samples": 20
            },
            "layer_metrics": [
                {"layer": i, "AD": np.random.normal(0.05 + 0.002*i, 0.08), "FOC": np.random.beta(3, 4),
                 "FOC_low": max(0, np.random.beta(3, 4) - 0.08), "FOC_high": min(1, np.random.beta(3, 4) + 0.08)}
                for i in range(32)
            ]
        }
    ]

    print(f"Generated sample data for {len(sample_results)} models")

    # Create enhanced visualization
    print("Creating enhanced layer importance plot...")
    fig1 = create_enhanced_layer_importance_plot(
        results_list=sample_results,
        metrics=['AD', 'FOC'],
        figsize=(16, 8),
        confidence_level=0.95,
        show_stats=True,
        save_path="enhanced_layer_importance.png",
        dpi=300
    )

    if fig1:
        plt.show()

    # Create correlation heatmap
    print("Creating correlation heatmap...")
    fig2 = create_correlation_heatmap(
        results_list=sample_results,
        save_path="layer_correlations.png"
    )

    if fig2:
        plt.show()

    # Generate statistical summary
    print("Generating statistical summary...")
    summary_df = generate_statistical_summary(sample_results)
    print("\nStatistical Summary:")
    print(summary_df.to_string(index=False))

    # Save summary
    summary_df.to_csv("layer_importance_statistical_summary.csv", index=False)
    print("âœ“ Statistical summary saved to layer_importance_statistical_summary.csv")

    print("\nVisualization demo completed successfully!")
    print("Generated files:")
    print("- enhanced_layer_importance.png: Main regression analysis plot")
    print("- layer_correlations.png: Correlation heatmap")
    print("- layer_importance_statistical_summary.csv: Detailed statistics")


if __name__ == "__main__":
    # Run the demonstration
    main_visualization_demo()

    print("\n" + "=" * 70)
    print("USAGE INSTRUCTIONS")
    print("=" * 70)
    print("""
