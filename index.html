<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probing the Origins of Reasoning Performance | Algoverse AI Research</title>
    <meta name="description" content="Representational quality for mathematical problem-solving in RL vs SFT finetuned models.">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <header class="header">
        <h1>Probing the Origins of Reasoning Performance: Representational Quality for Mathematical Problem-Solving in RL vs SFT Finetuned Models</h1>

        <div class="authors">
            <div class="author-list">
                <div class="author-item">
                    <div class="author-name">Antyabha Rahman<sup>*†</sup></div>
                    <div class="author-affiliation">University of New South Wales</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Omar Ankit<sup>†</sup></div>
                    <div class="author-affiliation">University of Waterloo</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Akshaj Gurugubelli<sup>†</sup></div>
                    <div class="author-affiliation">Algoverse AI Research</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Kevin Zhu</div>
                    <div class="author-affiliation">Algoverse AI Research</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Aishwarya Balwani<sup>†‡</sup></div>
                    <div class="author-affiliation">St. Jude Children's Research Hospital</div>
                </div>
            </div>
            <div class="footnote">
                <sup>*</sup>First author<br>
                <sup>†</sup>Work conducted with Algoverse AI Research<br>
                <sup>‡</sup>Corresponding author. Email: aishwarya.balwani@stjude.org
            </div>
        </div>

        <nav class="nav-bar">
            <a href="https://github.com/omara/reasoning-mechanistic-analysis" class="nav-button">
                <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
                    <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
                GitHub
            </a>
            <a href="#" class="nav-button">
                <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="M14 2H6C4.9 2 4 2.9 4 4v16c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V8l-6-6zM6 20V4h7v5h5v11H6z"></path>
                </svg>
                Paper (Coming Soon)
            </a>
            <a href="https://algoverse.ai" class="nav-button">
                Algoverse AI
            </a>
        </nav>
    </header>

    <div class="figure-section">
        <img src="images/figure3.png" alt="Accuracy Drop (AD) by model showing differential layer sensitivity">
    </div>

    <div class="container">
        <h2 class="abstract-title">Abstract</h2>
        <div class="content">
            <p>
                Large reasoning models trained via reinforcement learning (RL) substantially outperform their supervised counterparts
                on tasks requiring logic and mathematical reasoning, yet the mechanistic basis for these improvements remains unclear. 
                We investigate this phenomenon through an integrated behavioral-mechanistic analysis of mathematical reasoning, asking: 
                what internal differences enable RL models' superior performance? We present three converging lines of evidence that 
                RL models develop higher-quality reasoning representations earlier in their networks.
            </p>
            <p>
                First, linear probes trained on layer-wise hidden states reveal that RL models achieve an average probe accuracy of 
                82-87% compared to 71-76% for instruction-tuned models, representing an 11 percentage point improvement. Higher accuracy 
                in predicting answer correctness, with this capability emerging earlier than in base models—suggesting RL training produces 
                more structured, reasoning-relevant representations throughout the network. Second, soft-mean ablation studies show RL models 
                exhibit greater layer sensitivity overall, with impact increasing progressively toward deeper layers (r=0.47 vs r=-0.11 
                for instruction-tuned), confirming RL training creates hierarchical reasoning architecture with both earlier engagement 
                and deeper concentration.
            </p>
            <p>
                These representational differences manifest behaviorally in how models allocate computational resources during generation. 
                Analyzing within-problem token generation variability across 50 responses per problem, we find that Qwen3-Thinking maintains 
                remarkably consistent token usage (CV = 0.25-0.33) across all difficulty levels while achieving 97.7% accuracy, suggesting 
                its superior representations enable efficient, reliable solution paths. In contrast, DeepSeek-Math models exhibit elevated 
                variability specifically at capability boundaries (CV > 0.8 at 20-60% accuracy), indicating adaptive computational effort 
                where problems strain model capacity—yet DeepSeek's RL and supervised variants show nearly identical variability profiles, 
                suggesting current reward structures may not fully exploit the potential for adaptive token allocation.
            </p>
            <p>
                Together, these findings suggest that RL training fundamentally restructures how models represent and process reasoning 
                problems, building more robust and accessible reasoning representations. This mechanistic understanding provides actionable 
                insights for designing more reliable reasoning systems and highlights the value of probing internal representations to 
                understand capability improvements.
            </p>
        </div>
    </div>
    
    <div class="container">
        <section id="method">
            <h2>Methodology</h2>
            <p>
                Our investigation combines three complementary approaches to understand how RL training affects reasoning models:
                linear probing of layer-wise representations, soft-mean ablation studies to measure layer importance, and 
                analysis of computational resource allocation through token generation patterns.
            </p>
            
        </section>
    </div>

    <div class="figure-section">
        <img src="images/layer_performance_regression.png" alt="Layer depth vs accuracy showing probe performance across model families">
    </div>

    <div class="container">
        <section id="results">
            <h2>Computational Resource Allocation</h2>
            <p>
                Our analysis reveals how different models allocate computational resources during reasoning tasks,
                with distinct patterns emerging between RL-trained and supervised models.
            </p>
        </section>
    </div>

    <div class="figure-section">
        <img src="images/deepseekmath_cv_accc.png" alt="DeepSeek-Math models: Average output token CV by accuracy range">
    </div>

    <div class="figure-section">
        <img src="images/qwen_cv_accc.png" alt="Qwen models: Average output token CV by accuracy range">
    </div>

    <div class="container">
        <section id="additional">
        </section>
        
        
        <!-- Citation -->
        <!-- <section id="citation">
            <h2>Citation</h2>
            <p>If you find this work useful, please cite our paper:</p>
            <div class="citation-box">
                <pre><code>@article{rahman2025reasoning,
  title={Probing the Origins of Reasoning Performance: Representational Quality for Mathematical Problem-Solving in RL vs SFT Finetuned Models},
  author={Rahman, Antyabha and Ankit, Omar and Gurugubelli, Akshaj and Zhu, Kevin and Balwani, Aishwarya},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
            </div>
        </section> -->
    </div>

    <footer>
        <p>&copy; 2025 Algoverse AI Research. All rights reserved.</p>
    </footer>
</body>
</html>

