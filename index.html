<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probing the Origins of Reasoning Performance | Algoverse AI Research</title>
    <meta name="description" content="Representational quality for mathematical problem-solving in RL vs SFT finetuned models.">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header class="header">
        <h1>Probing the Origins of Reasoning Performance: Representational Quality for Mathematical Problem-Solving in RL vs SFT Finetuned Models</h1>

        <div class="authors">
            <div class="author-list">
                <div class="author-item">
                    <div class="author-name">Antyabha Rahman<sup>*†</sup></div>
                    <div class="author-affiliation">University of New South Wales</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Akshaj Gurugubelli<sup>*†</sup></div>
                    <div class="author-affiliation">Algoverse AI Research</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Omar Ankit<sup>*†</sup></div>
                    <div class="author-affiliation">University of Waterloo</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Kevin Zhu<sup>†</sup></div>
                    <div class="author-affiliation">Algoverse AI Research</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Ashwinee Panda<sup>†</sup></div>
                    <div class="author-affiliation">Algoverse AI Research</div>
                </div>
                <div class="author-item">
                    <div class="author-name">Aishwarya Balwani<sup>†‡</sup></div>
                    <div class="author-affiliation">St. Jude Children's Research Hospital</div>
                </div>
            </div>
            <div class="footnote">
                <sup>*</sup>Joint first co-authors with equal contributions. Listed in alphabetical order.<br>
                <sup>†</sup>Work conducted with Algoverse AI Research<br>
                <sup>‡</sup>Corresponding author. Email: aishwarya.balwani@stjude.org
            </div>
        </div>

        <nav class="nav-bar">
            <a href="https://github.com/omara/reasoning-mechanistic-analysis" class="nav-button">
                <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
                    <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
                GitHub
            </a>
            <a href="#" class="nav-button">
                <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="M14 2H6C4.9 2 4 2.9 4 4v16c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V8l-6-6zM6 20V4h7v5h5v11H6z"></path>
                </svg>
                ArXiv (Coming Soon)
            </a>
            <a href="https://algoverse.ai" class="nav-button">
                Algoverse AI
            </a>
        </nav>
    </header>

    <div class="figure-section">
        <img src="images/figure1.png" alt="Overview figure showing reasoning performance analysis">
    </div>

    <div class="container">
        <h2 class="abstract-title">Abstract</h2>
        <div class="content">
            <p>
                Large reasoning models trained via reinforcement learning (RL) substantially outperform their supervised counterparts
                on tasks requiring logic and mathematical reasoning, yet the mechanistic basis for these improvements remains unclear. 
                We investigate this phenomenon through an integrated behavioral-mechanistic analysis of mathematical reasoning, asking: 
                what internal differences enable RL models' superior performance? We present three converging lines of evidence that 
                RL models develop higher-quality reasoning representations earlier in their networks.
            </p>
            <p>
                First, linear probes trained on layer-wise hidden states reveal that RL models achieve an average probe accuracy of 
                82-87% compared to 71-76% for instruction-tuned models, representing an 11 percentage point improvement. Higher accuracy 
                in predicting answer correctness, with this capability emerging earlier than in base models—suggesting RL training produces 
                more structured, reasoning-relevant representations throughout the network. Second, soft-mean ablation studies show RL models 
                exhibit greater layer sensitivity overall, with impact increasing progressively toward deeper layers (r=0.47 vs r=-0.11 
                for instruction-tuned), confirming RL training creates hierarchical reasoning architecture with both earlier engagement 
                and deeper concentration.
            </p>
            <p>
                These representational differences manifest behaviorally in how models allocate computational resources during generation. 
                Analyzing within-problem token generation variability across 50 responses per problem, we find that Qwen3-Thinking maintains 
                remarkably consistent token usage (CV = 0.25-0.33) across all difficulty levels while achieving 97.7% accuracy, suggesting 
                its superior representations enable efficient, reliable solution paths. In contrast, DeepSeek-Math models exhibit elevated 
                variability specifically at capability boundaries (CV > 0.8 at 20-60% accuracy), indicating adaptive computational effort 
                where problems strain model capacity—yet DeepSeek's RL and supervised variants show nearly identical variability profiles, 
                suggesting current reward structures may not fully exploit the potential for adaptive token allocation.
            </p>
            <p>
                Together, these findings suggest that RL training fundamentally restructures how models represent and process reasoning 
                problems, building more robust and accessible reasoning representations. This mechanistic understanding provides actionable 
                insights for designing more reliable reasoning systems and highlights the value of probing internal representations to 
                understand capability improvements.
            </p>
        </div>
    </div>

    <div class="container">
        <!-- Key Findings -->
        <section id="findings">
            <h2>Key Findings</h2>
            
            <h3>1. RL Models Develop Higher-Quality Representations Earlier</h3>
            <div class="figure">
                <img src="images/figure1.png" alt="Layer-wise probe accuracy" class="content-img">
                <p class="caption"><strong>Figure 1:</strong> Layer-wise probe accuracy for predicting answer correctness across model families. Reasoning models (DeepSeek-Math-7B-RL, Qwen3-8B-Thinking) achieve higher probe accuracy (80–88%) and earlier emergence compared to base and instruction-tuned models (DeepSeek-Math-Instruct, Qwen3-8B Base) (55–75%). Notable late-layer regression appears in final layers for all models.</p>
            </div>
            
            <h3>2. Layer-Specific Impact on Reasoning Performance</h3>
            <div class="figure">
                <img src="images/figure2.png" alt="Accuracy drop by layer" class="content-img">
                <p class="caption"><strong>Figure 2:</strong> Accuracy Drop (AD) across layers for DeepSeek-Math-7B-Instruct and DeepSeek-Math-7B-RL. The RL model shows positive correlation with layer depth, while the instruction-tuned model exhibits negative correlation, demonstrating different architectural organizations of reasoning capabilities.</p>
            </div>
            
            <h3>3. Computational Resource Allocation Patterns</h3>
            <div class="figure">
                <img src="images/figure3.png" alt="Token coefficient of variation" class="content-img">
                <p class="caption"><strong>Figure 3:</strong> Token coefficient of variation by accuracy range across model families. (a) Qwen3-Thinking maintains consistent low variability (CV = 0.25–0.33) across all bins where data exists. (b) DeepSeek-Math models exhibit an inverted U-shape with peak variability in medium-difficulty regions (CV = 1.04 and 0.51 at 20–40%), demonstrating compression inefficiency at capability boundaries.</p>
            </div>
        </section>

        <!-- Additional Results -->
        <section id="results">
            <h2>Additional Results</h2>
            <p>
                Beyond our three main findings, we conducted extensive analysis to validate and extend our understanding 
                of how RL training affects reasoning representations and computational efficiency.
            </p>
            
            <div class="figure">
                <img src="images/figure5.png" alt="Comparative results" class="content-img">
                <p class="caption">Figure 5: Comparison of representation quality between RL and supervised models</p>
            </div>
            
            <div class="figure">
                <img src="images/figure6.png" alt="Computational efficiency" class="content-img">
                <p class="caption">Figure 6: Computational resource allocation patterns across model types</p>
            </div>
            
            <div class="figure">
                <img src="images/figure7.png" alt="Performance analysis" class="content-img">
                <p class="caption">Figure 7: Performance and variability analysis across difficulty levels</p>
            </div>
        </section>
        
        <!-- Citation -->
        <section id="citation">
            <h2>Citation</h2>
            <p>If you find this work useful, please cite our paper:</p>
            <div class="citation-box">
                <pre><code>@article{rahman2025reasoning,
  title={Probing the Origins of Reasoning Performance: Representational Quality for Mathematical Problem-Solving in RL vs SFT Finetuned Models},
  author={Rahman, Antyabha and Gurugubelli, Akshaj and Ankit, Omar and Zhu, Kevin and Panda, Ashwinee and Balwani, Aishwarya},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
            </div>
        </section>

        <footer>
            <p>&copy; 2025 Algoverse AI Research. All rights reserved.</p>
        </footer>
    </div>
</body>
</html>